#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=100000000000, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

sys.stderr.write("Training with IBM Model 1...\n")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]

# Expectation Maximization for IBM Model 1

# Initialize \theta_0 - A common choice is to initialize uniformly

sys.stderr.write("Initializing theta_0 uniformly")

theta = defaultdict(float)
e_count = defaultdict(int)
fe_count = defaultdict(int)

for (n, (f, e)) in enumerate(bitext):
  for f_i in set(f):
    for e_j in set(e):
      e_count[e_j] += 1
      fe_count[(f_i,e_j)] += 1
  if n % 500 == 0:
    sys.stderr.write(".")

for (k, (f, e)) in enumerate(fe_count.keys()):
    theta[(f,e)] = float(fe_count[(f,e)]) / float(e_count[e])

sys.stderr.write("theta_0 initialized - attempting to converge to values\n")

sys.stderr.write("Iterating 5 times\n")
for iteration in range(0,6):

  # Initialize all counts to 0
  e_count = defaultdict(float)
  fe_count = defaultdict(float)
  
  # E-Step: Compute expected counts
  for (n, (f, e)) in enumerate(bitext):    
      for f_i in f:
        norm = 0.
        for e_i in e:
          norm += theta[(f_i, e_i)]
        for e_i in e:
          c = theta[(f_i, e_i)] / norm
          fe_count[(f_i, e_i)] += c
          e_count[e_i] += c
      if n % 500 == 0:
        sys.stderr.write(".")

  sys.stderr.write("\n")

  # M-Step: Normalize
  for (k, (f, e)) in enumerate(fe_count.keys()):
    theta[(f,e)] = fe_count[(f,e)] / e_count[e]

  sys.stderr.write("Iteration complete\n")

for (f, e) in bitext:
  for (i, f_i) in enumerate(f): 
    for (j, e_j) in enumerate(e):
      if theta[(f_i,e_j)] >= opts.threshold:
        # print("prob of word",f_i,"exceeds threshold for",e_j,":",theta[(f_i,e_j)])
        sys.stdout.write("%i-%i " % (i,j))
  sys.stdout.write("\n")

